{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gettext import find\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.envs.registration\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import utils\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "Arr = np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, num_states: int, num_actions: int, start=0, terminal=None):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.start = start\n",
    "        if terminal is None:\n",
    "            self.terminal = np.array([], dtype=int)\n",
    "        else:\n",
    "            self.terminal = terminal\n",
    "        (self.T, self.R) = self.build()\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Constructs the T and R tensors from the dynamics of the environment.\n",
    "        Outputs:\n",
    "            T : (num_states, num_actions, num_states) State transition probabilities\n",
    "            R : (num_states, num_actions, num_states) Reward function\n",
    "        '''\n",
    "        num_states = self.num_states\n",
    "        num_actions = self.num_actions\n",
    "        T = np.zeros((num_states, num_actions, num_states))\n",
    "        R = np.zeros((num_states, num_actions, num_states))\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                (states, rewards, probs) = self.dynamics(s, a)\n",
    "                (all_s, all_r, all_p) = self.out_pad(states, rewards, probs)\n",
    "                T[s, a, all_s] = all_p\n",
    "                R[s, a, all_s] = all_r\n",
    "        return (T, R)\n",
    "\n",
    "    def dynamics(self, state: int, action: int) -> tuple[Arr, Arr, Arr]:\n",
    "        '''\n",
    "        Computes the distribution over possible outcomes for a given state\n",
    "        and action.\n",
    "        Inputs:\n",
    "            state : int (index of state)\n",
    "            action : int (index of action)\n",
    "        Outputs:\n",
    "            states  : (m,) all the possible next states\n",
    "            rewards : (m,) rewards for each next state transition\n",
    "            probs   : (m,) likelihood of each state-reward pair\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def render(pi: Arr):\n",
    "        '''\n",
    "        Takes a policy pi, and draws an image of the behavior of that policy,\n",
    "        if applicable.\n",
    "        Inputs:\n",
    "            pi : (num_actions,) a policy\n",
    "        Outputs:\n",
    "            None\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def out_pad(self, states: Arr, rewards: Arr, probs: Arr):\n",
    "        '''\n",
    "        Inputs:\n",
    "            states  : (m,) all the possible next states\n",
    "            rewards : (m,) rewards for each next state transition\n",
    "            probs   : (m,) likelihood of each state-reward pair\n",
    "        Outputs:\n",
    "            states  : (num_states,) all the next states\n",
    "            rewards : (num_states,) rewards for each next state transition\n",
    "            probs   : (num_states,) likelihood of each state-reward pair (including\n",
    "                           probability zero outcomes.)\n",
    "        '''\n",
    "        out_s = np.arange(self.num_states)\n",
    "        out_r = np.zeros(self.num_states)\n",
    "        out_p = np.zeros(self.num_states)\n",
    "        for i in range(len(states)):\n",
    "            idx = states[i]\n",
    "            out_r[idx] += rewards[i]\n",
    "            out_p[idx] += probs[i]\n",
    "        return (out_s, out_r, out_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toy(Environment):\n",
    "    def dynamics(self, state: int, action: int):\n",
    "        (S0, SL, SR) = (0, 1, 2)\n",
    "        LEFT = 0\n",
    "        num_states = 3\n",
    "        num_actions = 2\n",
    "        assert 0 <= state < self.num_states and 0 <= action < self.num_actions\n",
    "        if state == S0:\n",
    "            if action == LEFT:\n",
    "                (next_state, reward) = (SL, 1)\n",
    "            else:\n",
    "                (next_state, reward) = (SR, 0)\n",
    "        elif state == SL:\n",
    "            (next_state, reward) = (0, 0)\n",
    "        elif state == SR:\n",
    "            (next_state, reward) = (0, 2)\n",
    "        return (np.array([next_state]), np.array([reward]), np.array([1]))\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]]]\n",
      "[[[0. 1. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[2. 0. 0.]\n",
      "  [2. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "if MAIN:\n",
    "    toy = Toy()\n",
    "    print(toy.T)\n",
    "    print(toy.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norvig(Environment):\n",
    "    def dynamics(self, state: int, action: int) -> tuple[Arr, Arr, Arr]:\n",
    "        def state_index(state):\n",
    "            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)\n",
    "            pos = state[0] + state[1] * self.width\n",
    "            assert 0 <= pos < self.num_states, print(state, pos)\n",
    "            return pos\n",
    "\n",
    "        pos = self.states[state]\n",
    "        move = self.actions[action]\n",
    "        if state in self.terminal or state in self.walls:\n",
    "            return (np.array([state]), np.array([0]), np.array([1]))\n",
    "        out_probs = np.zeros(self.num_actions) + 0.1\n",
    "        out_probs[action] = 0.7\n",
    "        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions\n",
    "        out_rewards = np.zeros(self.num_actions) + self.penalty\n",
    "        new_states = [pos + x for x in self.actions]\n",
    "        for (i, s_new) in enumerate(new_states):\n",
    "            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):\n",
    "                out_states[i] = state\n",
    "                continue\n",
    "            new_state = state_index(s_new)\n",
    "            if new_state in self.walls:\n",
    "                out_states[i] = state\n",
    "            else:\n",
    "                out_states[i] = new_state\n",
    "            for idx in range(len(self.terminal)):\n",
    "                if new_state == self.terminal[idx]:\n",
    "                    out_rewards[i] = self.goal_rewards[idx]\n",
    "        return (out_states, out_rewards, out_probs)\n",
    "\n",
    "    def render(self, pi: Arr):\n",
    "        assert len(pi) == self.num_states\n",
    "        emoji = [\"⬆️\", \"➡️\", \"⬇️\", \"⬅️\"]\n",
    "        grid = [emoji[act] for act in pi]\n",
    "        grid[3] = \"🟩\"\n",
    "        grid[7] = \"🟥\"\n",
    "        grid[5] = \"⬛\"\n",
    "        print(str(grid[0:4]) + \"\\n\" + str(grid[4:8]) + \"\\n\" + str(grid[8:]))\n",
    "\n",
    "    def __init__(self, penalty=-0.04):\n",
    "        self.height = 3\n",
    "        self.width = 4\n",
    "        self.penalty = penalty\n",
    "        num_states = self.height * self.width\n",
    "        num_actions = 4\n",
    "        self.states = np.array([[x, y] for y in range(self.height) for x in range(self.width)])\n",
    "        self.actions = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])\n",
    "        self.dim = (self.height, self.width)\n",
    "        terminal = np.array([3, 7], dtype=int)\n",
    "        self.walls = np.array([5], dtype=int)\n",
    "        self.goal_rewards = np.array([1.0, -1])\n",
    "        super().__init__(num_states, num_actions, start=8, terminal=terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.7, 0. , 0. , 0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norvig = Norvig()\n",
    "pi = np.random.randint(norvig.num_actions, size=(norvig.num_states,))\n",
    "print(norvig.num_states)\n",
    "norvig.T[0,1] # probability of moving given state and action, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08, -0.04,  0.  ,  0.  , -0.04,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ,  0.  ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norvig.R[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_numerical(env: Environment, pi: Arr, gamma=0.99, eps=1e-08) -> Arr:\n",
    "    '''\n",
    "    Numerically evaluates the value of a given policy by iterating the Bellman equation\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        pi : shape (num_states,) - The policy to evaluate\n",
    "        gamma: float - Discount factor\n",
    "        eps  : float - Tolerance\n",
    "    Outputs:\n",
    "        value : float (num_states,) - The value function for policy pi\n",
    "    '''\n",
    "    v_s_old = np.zeros(env.num_states)\n",
    "    T = env.T # transition probs, shape (num_states, num_actions, num_states)\n",
    "    R = env.R # rewards, shape (num_states, num_actions, num_states)\n",
    "    while True:\n",
    "        v_s_new = np.zeros(env.num_states)\n",
    "        for s in range(env.num_states): # for all s in S\n",
    "            v_s_new[s] = np.dot(T[s,pi[s]], R[s,pi[s]] + gamma * v_s_old) # Bellman equation\n",
    "        if np.max(np.abs(v_s_new - v_s_old)) < eps:\n",
    "            break\n",
    "        v_s_old = v_s_new.copy()\n",
    "    return v_s_new\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_eval(policy_eval_numerical, exact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_exact(env: Environment, pi: Arr, gamma=0.99) -> Arr:\n",
    "    N = env.num_states\n",
    "    idx = np.arange(N)\n",
    "    p_i_j = env.T[idx,pi[idx],:]\n",
    "    r_i_j = env.R[idx,pi[idx],:]\n",
    "    r_i_pi = (p_i_j @ r_i_j.T).diagonal()\n",
    "    I = np.eye(N)\n",
    "\n",
    "    return np.linalg.inv(I - gamma*p_i_j) @ r_i_pi\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_eval(policy_eval_exact, exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env: Environment, V: Arr, gamma=0.99) -> Arr:\n",
    "    '''\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        V  : (num_states,) value of each state following some policy pi\n",
    "    Outputs:\n",
    "        pi_better : vector (num_states,) of actions representing a new policy obtained via policy iteration\n",
    "    '''\n",
    "    return (env.T * (env.R + V)).sum(axis = 2).argmax(axis = 1)\n",
    "if MAIN:\n",
    "    utils.test_policy_improvement(policy_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['➡️', '➡️', '➡️', '🟩']\n",
      "['⬆️', '⬛', '⬆️', '🟥']\n",
      "['⬆️', '➡️', '⬆️', '⬅️']\n"
     ]
    }
   ],
   "source": [
    "def find_optimal_policy(env: Environment, gamma=0.99):\n",
    "    '''\n",
    "    Inputs:\n",
    "        env: environment\n",
    "    Outputs:\n",
    "        pi : (num_states,) int, of actions represeting an optimal policy\n",
    "    '''\n",
    "    old_pi = np.random.randint(0, env.num_actions, env.num_states)\n",
    "    new_pi = np.random.randint(0, env.num_actions, env.num_states)\n",
    "    while (old_pi != new_pi).any():\n",
    "        old_pi = new_pi\n",
    "        val = policy_eval_exact(env, new_pi, gamma)\n",
    "        new_pi = policy_improvement(env, val, gamma)\n",
    "\n",
    "    return new_pi\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_find_optimal_policy(find_optimal_policy)\n",
    "    penalty = -0.1\n",
    "    norvig = Norvig(penalty)\n",
    "    pi_opt = find_optimal_policy(norvig, gamma=0.99)\n",
    "    norvig.render(pi_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b7e6b471291409f54dffbfbdfeccd6f1f2b5fb302e7acf62f723cf276419720"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
