{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from distutils.util import strtobool\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Discrete\n",
    "from typing import Any, List, Optional, Union, Tuple, Iterable\n",
    "from einops import rearrange\n",
    "from utils import ppo_parse_args, make_env\n",
    "import solutions\n",
    "from tests import test_agent, test_compute_advantages, test_calc_policy_loss, test_calc_value_function_loss, test_calc_entropy_loss, test_minibatch_indexes\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    t.nn.init.orthogonal_(layer.weight, std)\n",
    "    t.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    critic: nn.Sequential\n",
    "    actor: nn.Sequential\n",
    "\n",
    "    def __init__(self, envs: gym.vector.SyncVectorEnv, hidden_size = 64):\n",
    "        super().__init__()\n",
    "        obs_space_size = envs.single_observation_space.shape[0]\n",
    "        action_space_size = envs.single_action_space.n\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_space_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, action_space_size), std = 0.01)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_space_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, 1), std=1)\n",
    "        )\n",
    "envs = gym.vector.SyncVectorEnv([make_env(\"CartPole-v1\", i, i, False, \"test-run\") for i in range(5)])\n",
    "agent = Agent(envs)\n",
    "agent.actor\n",
    "test_agent(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@t.inference_mode()\n",
    "def compute_advantages(\n",
    "    next_value: t.Tensor,\n",
    "    next_done: t.Tensor,\n",
    "    rewards: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    dones: t.Tensor,\n",
    "    device: t.device,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> t.Tensor:\n",
    "    '''Compute advantages using Generalized Advantage Estimation.\n",
    "\n",
    "    next_value: shape (1, env) - represents V(s_{t+1}) which is needed for the last advantage term\n",
    "    next_done: shape (env,)\n",
    "    rewards: shape (t, env)\n",
    "    values: shape (t, env)\n",
    "    dones: shape (t, env)\n",
    "\n",
    "    Return: shape (t, env)\n",
    "    '''\n",
    "    T, _ = dones.shape\n",
    "    \n",
    "    # offset next values so we don't need to adjust indices\n",
    "    next_values = torch.concat([values[1:], next_value])\n",
    "    next_dones = torch.concat([dones[1:], next_done.unsqueeze(0)])\n",
    "\n",
    "    # if done, then no next values/future rewards\n",
    "    deltas = rewards + gamma * next_values * (1 - next_dones) - values\n",
    "\n",
    "    advantage_t = deltas.clone().to(device)\n",
    "    \n",
    "    for t in reversed(range(1,T)):\n",
    "        advantage_t[t-1] =  deltas[t-1] + gamma * gae_lambda * (1.0-dones[t]) * advantage_t[t]\n",
    "\n",
    "    return advantage_t \n",
    "\n",
    "test_compute_advantages(compute_advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_minibatch_indexes` passed.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Minibatch:\n",
    "    obs: t.Tensor\n",
    "    logprobs: t.Tensor\n",
    "    actions: t.Tensor\n",
    "    advantages: t.Tensor\n",
    "    returns: t.Tensor\n",
    "    values: t.Tensor\n",
    "\n",
    "def minibatch_indexes(batch_size: int, minibatch_size: int) -> list[np.ndarray]:\n",
    "    '''Return a list of length (batch_size // minibatch_size) where each element is an array of indexes into the batch.\n",
    "\n",
    "    Each index should appear exactly once.\n",
    "    '''\n",
    "    assert batch_size % minibatch_size == 0\n",
    "    indices = np.arange(batch_size)\n",
    "    np.random.shuffle(indices)\n",
    "    return indices.reshape(-1, minibatch_size).tolist()\n",
    "\n",
    "\n",
    "def make_minibatches(\n",
    "    obs: t.Tensor,\n",
    "    logprobs: t.Tensor,\n",
    "    actions: t.Tensor,\n",
    "    advantages: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    obs_shape: tuple,\n",
    "    action_shape: tuple,\n",
    "    batch_size: int,\n",
    "    minibatch_size: int,\n",
    ") -> list[Minibatch]:\n",
    "    '''Flatten the environment and steps dimension into one batch dimension, then shuffle and split into minibatches.'''\n",
    "    obs = obs.reshape(-1, *obs_shape)\n",
    "    logprobs = logprobs.reshape(-1)\n",
    "    actions = actions.reshape(-1, *action_shape)\n",
    "    advantages = advantages.reshape(-1)\n",
    "    values = values.reshape(-1)\n",
    "    returns = values + advantages\n",
    "\n",
    "    list_of_indices = minibatch_indexes(batch_size=batch_size, minibatch_size=minibatch_size)\n",
    "\n",
    "    minibatches = []\n",
    "    for indices in list_of_indices:\n",
    "        minibatches.append(Minibatch(\n",
    "            obs=obs[indices],\n",
    "            logprobs=logprobs[indices],\n",
    "            actions=actions[indices],\n",
    "            advantages=advantages[indices],\n",
    "            returns=returns[indices],\n",
    "            values=values[indices],\n",
    "        ))\n",
    "    return minibatches\n",
    "\n",
    "test_minibatch_indexes(minibatch_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_calc_policy_loss` passed.\n"
     ]
    }
   ],
   "source": [
    "def calc_policy_loss(\n",
    "    probs: Categorical, mb_action: t.Tensor, mb_advantages: t.Tensor, mb_logprobs: t.Tensor, clip_coef: float\n",
    ") -> t.Tensor:\n",
    "    '''Return the policy loss, suitable for maximisation with gradient ascent.\n",
    "\n",
    "    probs: a distribution containing the actor's unnormalized logits of shape (minibatch, num_actions)\n",
    "\n",
    "    clip_coef: amount of clipping, denoted by epsilon in Eq 7.\n",
    "\n",
    "    normalize: if true, normalize mb_advantages to have mean 0, variance 1\n",
    "\n",
    "    '''\n",
    "\n",
    "    # normalize advantages\n",
    "    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "    # get log probs of old model\n",
    "    logprobs_old = probs.log_prob(mb_action)\n",
    "\n",
    "    # calculated likelihood ratio\n",
    "    ratio = torch.exp(logprobs_old- mb_logprobs)\n",
    "\n",
    "    # calculate clipped likelihood ratio weighted mb_advantages\n",
    "    clip_adv = torch.clamp(ratio, 1-clip_coef, 1+clip_coef)\n",
    "\n",
    "    # calculate policy loss (whatever is smaller, grad itself or clipped grad)\n",
    "    loss = torch.min(ratio * mb_advantages, clip_adv * mb_advantages).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "test_calc_policy_loss(calc_policy_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_calc_value_function_loss` passed!\n"
     ]
    }
   ],
   "source": [
    "def calc_value_function_loss(critic: nn.Sequential, mb_obs: t.Tensor, mb_returns: t.Tensor, v_coef: float) -> t.Tensor:\n",
    "    '''Compute the value function portion of the loss function.\n",
    "\n",
    "    v_coef: the coefficient for the value loss, which weights its contribution to the overall loss. Denoted by c_1 in the paper.\n",
    "    '''\n",
    "    loss = (critic(mb_obs) - mb_returns).pow(2).mean() / 2\n",
    "    return loss*v_coef\n",
    "\n",
    "test_calc_value_function_loss(calc_value_function_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy_loss(probs: Categorical, ent_coef: float):\n",
    "    '''Return the entropy loss term.\n",
    "\n",
    "    ent_coef: the coefficient for the entropy loss, which weights its contribution to the overall loss. Denoted by c_2 in the paper.\n",
    "    '''\n",
    "    return probs.entropy().mean()*ent_coef\n",
    "\n",
    "test_calc_entropy_loss(calc_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "class PPOScheduler:\n",
    "    def __init__(self, optimizer, initial_lr: float, end_lr: float, num_updates: int):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.num_updates = num_updates\n",
    "        self.n_step_calls = 0\n",
    "\n",
    "    def step(self):\n",
    "        '''Implement linear learning rate decay so that after num_updates calls to step, the learning rate is end_lr.'''\n",
    "        if self.n_step_calls <= self.num_updates:\n",
    "            self.optimizer.param_groups[0]['lr'] = self.initial_lr + self.n_step_calls*(self.end_lr - self.initial_lr)/self.num_updates\n",
    "            self.n_step_calls+=1\n",
    "\n",
    "def make_optimizer(agent: Agent, num_updates: int, initial_lr: float, end_lr: float) -> tuple[optim.Adam, PPOScheduler]:\n",
    "    '''Return an appropriately configured Adam with its attached scheduler.'''\n",
    "    params = chain(agent.actor.parameters(), agent.actor.parameters())\n",
    "    optimizer = optim.Adam(params, lr = initial_lr)\n",
    "    return optimizer, PPOScheduler(optimizer, initial_lr, end_lr, num_updates)\n",
    "\n",
    "optimizer, scheduler = make_optimizer(agent, 100, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try running this file from the command line instead: python <filename of this script> --help\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1075\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1074\u001b[0m wi \u001b[39m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1075\u001b[0m wi\u001b[39m.\u001b[39;49msetup(kwargs)\n\u001b[1;32m   1076\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:165\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprinter\u001b[39m.\u001b[39mdisplay(line, level\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wl \u001b[39m=\u001b[39m wandb_setup\u001b[39m.\u001b[39;49msetup()\n\u001b[1;32m    166\u001b[0m \u001b[39m# Make sure we have a logger setup (might be an early logger)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py:312\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(settings)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(settings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39m\"\u001b[39m\u001b[39m_WandbSetup\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 312\u001b[0m     ret \u001b[39m=\u001b[39m _setup(settings\u001b[39m=\u001b[39;49msettings)\n\u001b[1;32m    313\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py:307\u001b[0m, in \u001b[0;36m_setup\u001b[0;34m(settings, _reset)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m wl \u001b[39m=\u001b[39m _WandbSetup(settings\u001b[39m=\u001b[39;49msettings)\n\u001b[1;32m    308\u001b[0m \u001b[39mreturn\u001b[39;00m wl\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py:293\u001b[0m, in \u001b[0;36m_WandbSetup.__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m _WandbSetup\u001b[39m.\u001b[39m_instance \u001b[39m=\u001b[39m _WandbSetup__WandbSetup(settings\u001b[39m=\u001b[39;49msettings, pid\u001b[39m=\u001b[39;49mpid)\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py:100\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup.__init__\u001b[0;34m(self, pid, settings, environ)\u001b[0m\n\u001b[1;32m     98\u001b[0m _set_logger(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_early_logger)\n\u001b[0;32m--> 100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings_setup(settings, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_early_logger)\n\u001b[1;32m    101\u001b[0m \u001b[39m# self._settings.freeze()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py:128\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup._settings_setup\u001b[0;34m(self, settings, early_logger)\u001b[0m\n\u001b[1;32m    126\u001b[0m     s\u001b[39m.\u001b[39m_apply_setup(settings, _logger\u001b[39m=\u001b[39mearly_logger)\n\u001b[0;32m--> 128\u001b[0m s\u001b[39m.\u001b[39;49m_infer_settings_from_environment()\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s\u001b[39m.\u001b[39m_cli_only_mode:\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_settings.py:1365\u001b[0m, in \u001b[0;36mSettings._infer_settings_from_environment\u001b[0;34m(self, _logger)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jupyter \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotebook_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotebook_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1365\u001b[0m     meta \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39;49mjupyter\u001b[39m.\u001b[39;49mnotebook_metadata(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msilent)\n\u001b[1;32m   1366\u001b[0m     settings[\u001b[39m\"\u001b[39m\u001b[39m_jupyter_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m meta\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/jupyter.py:227\u001b[0m, in \u001b[0;36mnotebook_metadata\u001b[0;34m(silent)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    222\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m/kaggle/working\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m: ipynb[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    224\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: ipynb[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    225\u001b[0m         }\n\u001b[0;32m--> 227\u001b[0m jupyter_metadata \u001b[39m=\u001b[39m notebook_metadata_from_jupyter_servers_and_kernel_id()\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m jupyter_metadata:\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/jupyter.py:163\u001b[0m, in \u001b[0;36mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt query password protected kernel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m res \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m    164\u001b[0m     urljoin(s[\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m\"\u001b[39;49m\u001b[39mapi/sessions\u001b[39;49m\u001b[39m\"\u001b[39;49m), params\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m\"\u001b[39;49m: s\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)}\n\u001b[1;32m    165\u001b[0m )\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    166\u001b[0m \u001b[39mfor\u001b[39;00m nn \u001b[39min\u001b[39;00m res:\n\u001b[1;32m    167\u001b[0m     \u001b[39m# TODO: wandb/client#400 found a case where res returned an array of\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# strings...\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 186\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     args \u001b[39m=\u001b[39m ppo_parse_args()\n\u001b[0;32m--> 186\u001b[0m train_ppo(args)\n",
      "Cell \u001b[0;32mIn [44], line 32\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mtrack:\n\u001b[1;32m     30\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     wandb\u001b[39m.\u001b[39;49minit(\n\u001b[1;32m     33\u001b[0m         project\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mwandb_project_name,\n\u001b[1;32m     34\u001b[0m         entity\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mwandb_entity,\n\u001b[1;32m     35\u001b[0m         sync_tensorboard\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     36\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mvars\u001b[39;49m(args),\n\u001b[1;32m     37\u001b[0m         name\u001b[39m=\u001b[39;49mrun_name,\n\u001b[1;32m     38\u001b[0m         monitor_gym\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     39\u001b[0m         save_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mruns/\u001b[39m\u001b[39m{\u001b[39;00mrun_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m writer\u001b[39m.\u001b[39madd_text(\n\u001b[1;32m     43\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhyperparameters\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m|param|value|\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m|-|-|\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m (key, value) \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(args)\u001b[39m.\u001b[39mitems()]),\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/arena/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1098\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[1;32m   1099\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[1;32m   1100\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class PPOArgs:\n",
    "    exp_name: str = \"PPO_first_attempt\"#os.path.basename(__file__).rstrip(\".py\")\n",
    "    seed: int = 1\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    track: bool = True\n",
    "    wandb_project_name: str = \"PPOCart\"\n",
    "    wandb_entity: str = None\n",
    "    capture_video: bool = False\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    total_timesteps: int = 500000\n",
    "    learning_rate: float = 0.00025\n",
    "    num_envs: int = 4\n",
    "    num_steps: int = 128\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    num_minibatches: int = 4\n",
    "    update_epochs: int = 4\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    batch_size: int = 512\n",
    "    minibatch_size: int = 128\n",
    "\n",
    "def train_ppo(args):\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join([f\"|{key}|{value}|\" for (key, value) in vars(args).items()]),\n",
    "    )\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "    action_shape = envs.single_action_space.shape\n",
    "    assert action_shape is not None\n",
    "    assert isinstance(envs.single_action_space, Discrete), \"only discrete action space is supported\"\n",
    "    agent = Agent(envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "    (optimizer, scheduler) = make_optimizer(agent, num_updates, args.learning_rate, 0.0)\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + action_shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    global_step = 0\n",
    "    old_approx_kl = 0.0\n",
    "    approx_kl = 0.0\n",
    "    value_loss = t.tensor(0.0)\n",
    "    policy_loss = t.tensor(0.0)\n",
    "    entropy_loss = t.tensor(0.0)\n",
    "    clipfracs = []\n",
    "    info = []\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    for _ in range(num_updates):\n",
    "        for i in range(0, args.num_steps):\n",
    "            \"YOUR CODE: Rollout phase (see detail #1)\"\n",
    "\n",
    "            global_step += args.num_envs\n",
    "\n",
    "            obs[i] = next_obs # we defined obs as 0's so we fill it in\n",
    "            dones[i] = next_done # we defined dones as 0's so we fill it in\n",
    "            \n",
    "            # critic provides values of states,\n",
    "            # actor provides dist over actions\n",
    "            with t.inference_mode():\n",
    "                next_values = agent.critic(next_obs).flatten()\n",
    "                logits = agent.actor(next_obs)\n",
    "\n",
    "            # pick an actual action though\n",
    "            probs = Categorical(logits = logits)\n",
    "            action = probs.sample() # wait\n",
    "            next_obs, reward, done, info = envs.step(\n",
    "                actions[i].cpu().numpy()\n",
    "            )\n",
    "\n",
    "            # store all the results, casting from numpy where appropriate.\n",
    "            rewards[i] = t.from_numpy(reward)\n",
    "            logprobs[i] = probs.log_prob(action)\n",
    "            actions[i] = action\n",
    "            values[i] = next_values\n",
    "\n",
    "            next_obs = t.from_numpy(next_obs).to(device)\n",
    "            next_done = t.from_numpy(done).to(device())\n",
    "\n",
    "            for item in info:\n",
    "                if \"episode\" in item.keys():\n",
    "                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                    break\n",
    "\n",
    "        next_value = rearrange(agent.critic(next_obs), \"env 1 -> 1 env\")\n",
    "        advantages = compute_advantages(\n",
    "            next_value, next_done, rewards, values, dones, device, args.gamma, args.gae_lambda\n",
    "        )\n",
    "        clipfracs.clear()\n",
    "        for _ in range(args.update_epochs):\n",
    "            minibatches = make_minibatches(\n",
    "                obs,\n",
    "                logprobs,\n",
    "                actions,\n",
    "                advantages,\n",
    "                values,\n",
    "                envs.single_observation_space.shape,\n",
    "                action_shape,\n",
    "                args.batch_size,\n",
    "                args.minibatch_size,\n",
    "            )\n",
    "            for mb in minibatches:\n",
    "                \"YOUR CODE: compute loss on the minibatch and step the optimizer (not the scheduler). Do detail #11 (global gradient clipping) here using nn.utils.clip_grad_norm_.\"\n",
    "            \n",
    "                # ok so here we need to conpute the losses, let's do that.\n",
    "\n",
    "                # for starter, let's get the \"old probs\"\n",
    "                with t.inference_mode():\n",
    "                    logits = agent.actor(mb.obs)\n",
    "                    probs = Categorical(logits = logits)\n",
    "\n",
    "                value_loss = calc_value_function_loss(agent.critic, mb.obs, mb.returns, args.v_coef)\n",
    "                policy_loss = calc_policy_loss(probs, mb.actions, mb.advantages, mb.logprobs, args.clip_coef)\n",
    "                entropy_loss = calc_entropy_loss(probs, args.ebt_coef)\n",
    "\n",
    "                # now get ready to do standard backward pass with losses\n",
    "                total_loss = policy_loss - calc_value_function_loss + entropy_loss\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters, args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "        (y_pred, y_true) = (mb.values.cpu().numpy(), mb.returns.cpu().numpy())\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        with torch.no_grad():\n",
    "            newlogprob: t.Tensor = probs.log_prob(mb.actions)\n",
    "            logratio = newlogprob - mb.logprobs\n",
    "            ratio = logratio.exp()\n",
    "            old_approx_kl = (-logratio).mean().item()\n",
    "            approx_kl = (ratio - 1 - logratio).mean().item()\n",
    "            clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl, global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        if global_step % 10 == 0:\n",
    "            print(\"steps per second (SPS):\", int(global_step / (time.time() - start_time)))\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "if MAIN:\n",
    "    if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n",
    "        filename = globals().get(\"__file__\", \"<filename of this script>\")\n",
    "        print(f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\")\n",
    "        args = PPOArgs()\n",
    "    else:\n",
    "        args = ppo_parse_args()\n",
    "    train_ppo(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b7e6b471291409f54dffbfbdfeccd6f1f2b5fb302e7acf62f723cf276419720"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
