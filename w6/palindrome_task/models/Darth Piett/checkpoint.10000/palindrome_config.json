{"n_layers": 3, "d_model": 32, "n_ctx": 11, "d_head": 16, "model_name": "custom", "n_heads": 2, "d_mlp": 64, "act_fn": "relu", "d_vocab": 11, "eps": 1e-05, "use_attn_result": false, "use_attn_scale": true, "use_local_attn": false, "original_architecture": null, "from_checkpoint": false, "checkpoint_index": null, "checkpoint_label_type": null, "checkpoint_value": null, "tokenizer_name": null, "window_size": null, "attn_types": null, "init_mode": "gpt2", "normalization_type": null, "device": "cpu", "attention_dir": "causal", "attn_only": false, "seed": null, "initializer_range": 0.1414213562373095, "init_weights": true, "scale_attn_by_inverse_layer_idx": false, "positional_embedding_type": "standard", "final_rms": false, "d_vocab_out": 64, "parallel_attn_mlp": false, "rotary_dim": null, "n_params": 24576}