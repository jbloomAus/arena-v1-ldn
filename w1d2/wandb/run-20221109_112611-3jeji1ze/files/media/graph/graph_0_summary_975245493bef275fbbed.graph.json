{"format": "torch", "nodes": [{"name": "token_embedding", "id": 5290266336, "class_name": "Embedding(10, 64)", "parameters": [["weight", [10, 64]]], "output_shape": [[1024, 6, 64]], "num_parameters": [640]}, {"name": "positional_embedding", "id": 5247277360, "class_name": "PositionalEncoding()", "parameters": [], "output_shape": [[1024, 6, 64]], "num_parameters": []}, {"name": "dropout", "id": 5274466144, "class_name": "Dropout(Dropout(p = 0.1)", "parameters": [], "output_shape": [[1024, 6, 64]], "num_parameters": []}, {"name": "decoder_blocks", "id": 5274702256, "class_name": "Sequential(\n  (0): DecoderBlock(\n    (MLP): MLP(\n      (linear_1): Linear(in_features 64, out_features 256. bias True)\n      (linear_2): Linear(in_features 256, out_features 64. bias True)\n      (gelu): GELU()\n      (dropout): Dropout(Dropout(p = 0.1)\n    )\n    (multiheaded_self_attention): MultiheadMaskedAttention(\n      (W_QKV): Linear(in_features=64, out_features=192, bias=True)\n      (W_O): Linear(in_features=64, out_features=64, bias=True)\n    )\n    (layer_norm_1): LayerNorm()\n    (layer_norm_2): LayerNorm()\n  )\n  (1): DecoderBlock(\n    (MLP): MLP(\n      (linear_1): Linear(in_features 64, out_features 256. bias True)\n      (linear_2): Linear(in_features 256, out_features 64. bias True)\n      (gelu): GELU()\n      (dropout): Dropout(Dropout(p = 0.1)\n    )\n    (multiheaded_self_attention): MultiheadMaskedAttention(\n      (W_QKV): Linear(in_features=64, out_features=192, bias=True)\n      (W_O): Linear(in_features=64, out_features=64, bias=True)\n    )\n    (layer_norm_1): LayerNorm()\n    (layer_norm_2): LayerNorm()\n  )\n)", "parameters": [["0.MLP.linear_1.weight", [256, 64]], ["0.MLP.linear_1.bias", [256]], ["0.MLP.linear_2.weight", [64, 256]], ["0.MLP.linear_2.bias", [64]], ["0.multiheaded_self_attention.W_QKV.weight", [192, 64]], ["0.multiheaded_self_attention.W_QKV.bias", [192]], ["0.multiheaded_self_attention.W_O.weight", [64, 64]], ["0.multiheaded_self_attention.W_O.bias", [64]], ["0.layer_norm_1.weight", [64]], ["0.layer_norm_1.bias", [64]], ["0.layer_norm_2.weight", [64]], ["0.layer_norm_2.bias", [64]], ["1.MLP.linear_1.weight", [256, 64]], ["1.MLP.linear_1.bias", [256]], ["1.MLP.linear_2.weight", [64, 256]], ["1.MLP.linear_2.bias", [64]], ["1.multiheaded_self_attention.W_QKV.weight", [192, 64]], ["1.multiheaded_self_attention.W_QKV.bias", [192]], ["1.multiheaded_self_attention.W_O.weight", [64, 64]], ["1.multiheaded_self_attention.W_O.bias", [64]], ["1.layer_norm_1.weight", [64]], ["1.layer_norm_1.bias", [64]], ["1.layer_norm_2.weight", [64]], ["1.layer_norm_2.bias", [64]]], "output_shape": [[1024, 6, 64]], "num_parameters": [16384, 256, 16384, 64, 12288, 192, 4096, 64, 64, 64, 64, 64, 16384, 256, 16384, 64, 12288, 192, 4096, 64, 64, 64, 64, 64]}, {"name": "layer_norm_final", "id": 5292198976, "class_name": "LayerNorm()", "parameters": [["weight", [64]], ["bias", [64]]], "output_shape": [[1024, 6, 64]], "num_parameters": [64, 64]}], "edges": []}